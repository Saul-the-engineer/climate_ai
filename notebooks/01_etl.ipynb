{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This notebook has the goal of opening and preprocessing the climate data associated with the midwest region of the United States. The data is composed of 9 datasets: \n",
    "\n",
    "* tp: Total precipitation\n",
    "* sst: Sea surface temperature\n",
    "* thickness: Tropospheric thickness between 850 and 500 hPa\n",
    "* pottemp: 45m ocean potential temperature\n",
    "* pr_wtr: Precipitable water\n",
    "* lftx4: Atmospheric lifted index\n",
    "* chi: upper troposphere velocity potential\n",
    "* tcdc: total cloud cover\n",
    "* shum: 2m specific humidity\n",
    "\n",
    "The data is available in the form of NetCDF files, which are opened using the `xarray` library. We have the initial challenge that the predictior and target datasets have different resolutions and alignments. This data needs to be resampled and subset in order for the analysis to take place. Because Sea Surface Temperature (SST) and Ocean Potential Temperature (pottemp) do not have any overlap with the area of interest, they are replaced with the Ocean Nino Index (ONI) which can give us context about global phenomena that affect precipitation.\n",
    "\n",
    "The preprocessing steps are as follows:\n",
    "1. Open the data\n",
    "2. Subset predictor data to the region of interest\n",
    "3. Resample precipitation data to the resolution of the predictor data\n",
    "4. Merge the datasets\n",
    "5. Save the data in a format that can be used in the next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_netcdf_metadata(dataset: xr.Dataset):\n",
    "    \"\"\"\n",
    "    Get the bounding box coordinates, origin, and resolution from a NetCDF dataset.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (xr.Dataset): The xarray dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing bounding box coordinates\n",
    "              (lat_bottom, lat_top, lon_left, lon_right).\n",
    "        tuple: The origin (first latitude, first longitude).\n",
    "        float: Latitude resolution.\n",
    "        float: Longitude resolution.\n",
    "    \"\"\"\n",
    "    # Get the latitude and longitude arrays (assuming geospatial data)\n",
    "    lat = dataset[\"latitude\"].values\n",
    "    lon = dataset[\"longitude\"].values  # same for 'longitude'\n",
    "\n",
    "    # Get bounding box coordinates\n",
    "    bounding_box = {\n",
    "        \"lat_bottom\": lat.min(),  # southern limit\n",
    "        \"lat_top\": lat.max(),  # northern limit\n",
    "        \"lon_left\": lon.min(),  # western limit\n",
    "        \"lon_right\": lon.max(),  # eastern limit\n",
    "    }\n",
    "\n",
    "    # Determine the origin (first lat/lon value)\n",
    "    origin = (lat[0], lon[0])\n",
    "\n",
    "    # Calculate resolution (difference between consecutive latitude and longitude values)\n",
    "    lat_resolution = abs(lat[1] - lat[0])\n",
    "    lon_resolution = abs(lon[1] - lon[0])\n",
    "\n",
    "    return bounding_box, origin, lat_resolution, lon_resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data directory\n",
    "data_dir = pathlib.Path(os.getcwd()).parent / \"data\"\n",
    "precip_file = data_dir / \"raw_data\" / \"monthly_us_midwest_precip.nc\"\n",
    "predictor_file = data_dir / \"raw_data\" / \"predictor_variables.nc\"\n",
    "oni_file = data_dir / \"raw_data\" / \"ocean_nino_index.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the datasets\n",
    "ds_precipitation = xr.open_dataset(precip_file)\n",
    "ds_predictors = xr.open_dataset(predictor_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the metadata\n",
    "precip_bounding_box, precip_origin, precip_res_y, precip_res_x = get_netcdf_metadata(\n",
    "    ds_precipitation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the metadata\n",
    "pred_bounding_box, pred_origin, pred_res_y, pred_res_x = get_netcdf_metadata(\n",
    "    ds_predictors\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset predictor dataset with buffer\n",
    "ds_predictors_subset = ds_predictors.sel(\n",
    "    latitude=slice(\n",
    "        precip_bounding_box[\"lat_bottom\"],\n",
    "        precip_bounding_box[\"lat_top\"],\n",
    "    ),\n",
    "    longitude=slice(\n",
    "        precip_bounding_box[\"lon_left\"],\n",
    "        precip_bounding_box[\"lon_right\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the predictor dataset to 0.25° resolution using interpolation\n",
    "ds_precipitation_resampled = ds_precipitation.interp(\n",
    "    latitude=ds_predictors_subset[\"latitude\"],\n",
    "    longitude=ds_predictors_subset[\"longitude\"],\n",
    "    method=\"linear\",  # interpolation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the resampled predictors with the precipitation dataset\n",
    "combined_ds = xr.merge(\n",
    "    [\n",
    "        ds_precipitation_resampled,\n",
    "        ds_predictors_subset,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset> Size: 1MB\n",
      "Dimensions:    (time: 504, latitude: 5, longitude: 13)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 4kB 1982-01-01 1982-02-01 ... 2023-12-01\n",
      "  * latitude   (latitude) float32 20B 37.5 40.0 42.5 45.0 47.5\n",
      "  * longitude  (longitude) float32 52B -110.0 -107.5 -105.0 ... -82.5 -80.0\n",
      "    level      float32 4B 45.0\n",
      "Data variables:\n",
      "    tp         (time, latitude, longitude) float32 131kB 45.93 130.2 ... 26.05\n",
      "    sst        (time, latitude, longitude) float32 131kB ...\n",
      "    thickness  (time, latitude, longitude) float32 131kB ...\n",
      "    pottmp     (time, latitude, longitude) float32 131kB ...\n",
      "    pr_wtr     (time, latitude, longitude) float32 131kB ...\n",
      "    lftx4      (time, latitude, longitude) float32 131kB ...\n",
      "    chi        (time, latitude, longitude) float32 131kB ...\n",
      "    tcdc       (time, latitude, longitude) float32 131kB ...\n",
      "    shum       (time, latitude, longitude) float32 131kB ...\n"
     ]
    }
   ],
   "source": [
    "print(combined_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Remove null dataset because it doesn't overlap with region of interest\n",
    "combined_ds = combined_ds.drop_vars([\"sst\", \"pottmp\"])\n",
    "\n",
    "# Save the combined dataset to a new netCDF file\n",
    "combined_ds.to_netcdf(data_dir / \"processed_data\" / \"combined_data.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cold & Warm Episodes by Season\n",
    "This section contains the Oceanic Niño Index (ONI) values for each month, which are used to identify warm and cold episodes associated with the El Niño-Southern Oscillation (ENSO).\n",
    "\n",
    "Notice: Due to the high-frequency filter applied to the ERSSTv5 data (Huang et al., 2017, Journal of Climate), ONI values may change up to two months after the initial \"real-time\" value is posted. Therefore, the most recent ONI values should be regarded as estimates.\n",
    "\n",
    "Description\n",
    "The ONI indicates warm (red) and cold (blue) periods based on a threshold of ±0.5°C. It represents a 3-month running mean of ERSST.v5 SST anomalies in the Niño 3.4 region (5°N–5°S, 120°–170°W), calculated using centered 30-year base periods that are updated every 5 years.\n",
    "\n",
    "For historical purposes, periods of below-normal and above-normal sea surface temperatures (SSTs) are color-coded in blue and red, respectively, when the threshold is met for a minimum of 5 consecutive overlapping seasons.\n",
    "\n",
    "The ONI is one measure of the El Niño-Southern Oscillation, and other indices can confirm whether features consistent with a coupled ocean-atmosphere phenomenon accompany these periods.\n",
    "\n",
    "For more information, visit the NOAA CPC ONI page. https://origin.cpc.ncep.noaa.gov/products/analysis_monitoring/ensostuff/ONI_v5.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load oni data\n",
    "def oni_pipeline(filepath: pathlib.Path):\n",
    "    # read data\n",
    "    df = pd.read_csv(filepath, header=0, index_col=0, sep=\",\")\n",
    "\n",
    "    # remove rows with \"Year\" in index\n",
    "    df = df[df.index != \"Year\"]\n",
    "\n",
    "    # flatten dataframe to 1 column\n",
    "    df = df.stack().reset_index()\n",
    "\n",
    "    # make pandas datetime index from january 1950 to december 2022\n",
    "    df.index = pd.date_range(start=\"1950-01-01\", end=\"2024-07-01\", freq=\"MS\")\n",
    "\n",
    "    # drop columns\n",
    "    df.drop([\"Year\", \"level_1\"], axis=1, inplace=True)\n",
    "\n",
    "    # rename columns\n",
    "    df.rename(columns={0: \"oni\"}, inplace=True)\n",
    "\n",
    "    # make enso column numeric\n",
    "    df[\"oni\"] = pd.to_numeric(df[\"oni\"]).astype(float)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean ONI dataset and save to CSV\n",
    "oni = oni_pipeline(oni_file)\n",
    "oni.to_csv(data_dir / \"processed_data\" / \"ocean_nino_index.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
